"""
é€‚é…å™¨ï¼šå°†è‡ªå®šä¹‰ LlamaForCausalLM æˆ– GPTNeoXForCausalLM æ¨¡å‹åŒ…è£…æˆå…¼å®¹ TransformerLens API çš„æ ¼å¼

è¿™ä¸ªé€‚é…å™¨å…è®¸ä½ åœ¨ head_detect_llama_jx.py ä¸­ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹ï¼Œ
è€Œä¸éœ€è¦ TransformerLens çš„ HookedTransformerã€‚

æ”¯æŒä¸¤ç§æ¨¡å‹ç±»å‹ï¼š
- LlamaForCausalLM (éœ€è¦è‡ªå®šä¹‰å®ç°)
- GPTNeoXForCausalLM (æ ‡å‡† HuggingFace å®ç°)
"""

import torch
from typing import Union, List, Optional
from transformers import AutoTokenizer, GPTNeoXForCausalLM
import sys
from pathlib import Path

# æ·»åŠ è·¯å¾„ä»¥å¯¼å…¥è‡ªå®šä¹‰æ¨¡å‹å’Œ hook_utils
# _current_dir = Path(__file__).parent
# _retrieval_head_dir = _current_dir.parent.parent / "Retrieval_Head"
# sys.path.insert(0, str(_retrieval_head_dir / "faiss_attn"))
# sys.path.insert(0, str(_retrieval_head_dir))
import sys
sys.path.append("./faiss_attn/")

try:
    from source.modeling_llama import LlamaForCausalLM as CustomLlamaForCausalLM
    HAS_CUSTOM_LLAMA = True
except ImportError:
    HAS_CUSTOM_LLAMA = False
    CustomLlamaForCausalLM = None

# å¯¼å…¥ hook_utils
try:
    from hook_utils import run_with_cache, ActivationCache
except ImportError as e:
    raise ImportError(
        f"æ— æ³•å¯¼å…¥ hook_utilsã€‚è¯·ç¡®ä¿è·¯å¾„æ­£ç¡®ã€‚\n"
    )


class ModelConfig:
    """æ¨¡æ‹Ÿ TransformerLens çš„ cfg å¯¹è±¡"""
    def __init__(self, model):
        self.n_layers = model.config.num_hidden_layers
        self.n_heads = model.config.num_attention_heads
        self.n_ctx = model.config.max_position_embeddings
        # è·å–è®¾å¤‡
        if hasattr(model, 'device'):
            self.device = model.device
        elif next(model.parameters()).is_cuda:
            self.device = "cuda"
        else:
            self.device = "cpu"


class CustomModelAdapter:
    """
    å°†è‡ªå®šä¹‰ LlamaForCausalLM æˆ– GPTNeoXForCausalLM æ¨¡å‹é€‚é…æˆå…¼å®¹ TransformerLens API çš„æ ¼å¼
    
    ä½¿ç”¨ç¤ºä¾‹:
        from model_adapter import CustomModelAdapter
        
        # åŠ è½½ LLaMA æ¨¡å‹
        model = CustomModelAdapter.from_pretrained(
            "meta-llama/Llama-2-7b-hf",
            device="cuda",
            torch_dtype=torch.bfloat16
        )
        
        # æˆ–åŠ è½½ GPTNeoX/Pythia æ¨¡å‹
        model = CustomModelAdapter.from_pretrained(
            "EleutherAI/pythia-6.9b-deduped",
            device="cuda",
            torch_dtype=torch.bfloat16,
            revision="step3000"
        )
        
        # ç°åœ¨å¯ä»¥åƒä½¿ç”¨ HookedTransformer ä¸€æ ·ä½¿ç”¨
        tokens = model.to_tokens("Hello world")
        logits, cache = model.run_with_cache(tokens, remove_batch_dim=True)
        attn_pattern = cache["pattern", 0, "attn"]
    """
    
    def __init__(self, model: Union[CustomLlamaForCausalLM, GPTNeoXForCausalLM], tokenizer=None):
        """
        Args:
            model: LlamaForCausalLM æˆ– GPTNeoXForCausalLM æ¨¡å‹
            tokenizer: å¯é€‰çš„ tokenizerï¼ˆå¦‚æœä¸æä¾›ï¼Œä¼šä»æ¨¡å‹åç§°è‡ªåŠ¨åŠ è½½ï¼‰
        """
        self._model = model
        self._model.eval()  # ç¡®ä¿æ˜¯ eval æ¨¡å¼
        
        # åˆ›å»ºé…ç½®å¯¹è±¡
        self.cfg = ModelConfig(model)
        
        # åŠ è½½ tokenizer
        if tokenizer is None:
            # å°è¯•ä»æ¨¡å‹é…ç½®ä¸­è·å– tokenizer åç§°
            model_name = getattr(model.config, '_name_or_path', None)
            if model_name is None:
                # å°è¯•ä»æ¨¡å‹è·¯å¾„æ¨æ–­
                if hasattr(model, 'name_or_path'):
                    model_name = model.name_or_path
                else:
                    raise ValueError(
                        "æ— æ³•è‡ªåŠ¨æ¨æ–­ tokenizer åç§°ã€‚è¯·æä¾› tokenizer å‚æ•°ã€‚"
                    )
            
            # å¯¹äº Pythia æ¨¡å‹ï¼Œä½¿ç”¨ GPTNeoX tokenizerï¼ˆPythia ä½¿ç”¨ç›¸åŒçš„ tokenizerï¼‰
            is_pythia = "pythia" in model_name.lower()
            
            if is_pythia:
                # Pythia æ¨¡å‹ä½¿ç”¨ GPTNeoX tokenizer
                # ä½¿ç”¨åŸºç¡€ tokenizer æ¨¡å‹ï¼Œå› ä¸º Pythia çš„ tokenizer æ–‡ä»¶å¯èƒ½æœ‰é—®é¢˜
                tokenizer_model = "EleutherAI/gpt-neox-20b"
                print(f"ğŸ“¦ Detected Pythia model, using GPTNeoX tokenizer from: {tokenizer_model}")
                try:
                    # å°è¯•ä½¿ç”¨ fast tokenizer
                    self._tokenizer = AutoTokenizer.from_pretrained(tokenizer_model, use_fast=True)
                except Exception as e:
                    print(f"âš ï¸  Warning: Failed to load fast tokenizer: {e}")
                    try:
                        # å›é€€åˆ° slow tokenizer
                        self._tokenizer = AutoTokenizer.from_pretrained(tokenizer_model, use_fast=False)
                    except Exception as e2:
                        print(f"âš ï¸  Warning: Failed to load slow tokenizer: {e2}")
                        # æœ€åå°è¯•ç›´æ¥ä»æ¨¡å‹åŠ è½½ï¼Œä½†ä½¿ç”¨ use_fast=False
                        print(f"   Trying to load tokenizer directly from model...")
                        self._tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False, trust_remote_code=True)
            else:
                # é Pythia æ¨¡å‹ï¼Œä½¿ç”¨åŸæ¥çš„é€»è¾‘
                self._tokenizer = AutoTokenizer.from_pretrained(model_name)
        else:
            self._tokenizer = tokenizer
        
        # ç¡®ä¿ tokenizer æœ‰ pad_token
        if self._tokenizer.pad_token is None:
            self._tokenizer.pad_token = self._tokenizer.eos_token
        
        # æš´éœ² tokenizer ä½œä¸ºå…¬å…±å±æ€§ï¼ˆç”¨äºå…¼å®¹æ€§ï¼Œå¦‚ iteration_head_detectorï¼‰
        self.tokenizer = self._tokenizer
    
    def to_tokens(self, seq: Union[str, List[str]], prepend_bos: bool = True) -> torch.Tensor:
        """
        å°†æ–‡æœ¬è½¬æ¢ä¸º token IDsï¼Œå…¼å®¹ TransformerLens çš„ to_tokens æ–¹æ³•
        
        Args:
            seq: å­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨
            prepend_bos: æ˜¯å¦åœ¨å¼€å¤´æ·»åŠ  BOS tokenï¼ˆTransformerLens é»˜è®¤è¡Œä¸ºï¼‰
        
        Returns:
            Token IDs tensorï¼Œshape: [batch_size, seq_len]
        """
        if isinstance(seq, str):
            seq = [seq]
        
        # Tokenize
        encoded = self._tokenizer(
            seq,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=self.cfg.n_ctx
        )
        
        input_ids = encoded['input_ids']
        
        # TransformerLens é»˜è®¤ä¼š prepend_bosï¼Œä½†æˆ‘ä»¬çš„ tokenizer å¯èƒ½å·²ç»åŒ…å«äº†
        # æ£€æŸ¥ç¬¬ä¸€ä¸ª token æ˜¯å¦æ˜¯ BOS
        if prepend_bos and self._tokenizer.bos_token_id is not None:
            # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰ BOS token
            has_bos = (input_ids[:, 0] == self._tokenizer.bos_token_id).all()
            if not has_bos:
                # æ·»åŠ  BOS token
                bos_tensor = torch.full(
                    (input_ids.shape[0], 1),
                    self._tokenizer.bos_token_id,
                    dtype=input_ids.dtype,
                    device=input_ids.device
                )
                input_ids = torch.cat([bos_tensor, input_ids], dim=1)
        
        # ç§»åŠ¨åˆ°æ¨¡å‹è®¾å¤‡
        if hasattr(self._model, 'device'):
            input_ids = input_ids.to(self._model.device)
        elif next(self._model.parameters()).is_cuda:
            input_ids = input_ids.cuda()
        
        return input_ids
    
    def to_str_tokens(self, seq: Union[str, List[str], torch.Tensor], prepend_bos: bool = True) -> List[str]:
        """
        å°†æ–‡æœ¬æˆ– token IDs è½¬æ¢ä¸º token å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œå…¼å®¹ TransformerLens çš„ to_str_tokens æ–¹æ³•
        
        Args:
            seq: å­—ç¬¦ä¸²ã€å­—ç¬¦ä¸²åˆ—è¡¨æˆ– token IDs tensor
            prepend_bos: æ˜¯å¦åœ¨å¼€å¤´æ·»åŠ  BOS token
        
        Returns:
            Token å­—ç¬¦ä¸²åˆ—è¡¨
        """
        if isinstance(seq, torch.Tensor):
            # å¦‚æœæ˜¯ tensorï¼Œç›´æ¥è§£ç 
            tokens = seq.cpu().tolist()
            if isinstance(tokens[0], list):
                # batch æƒ…å†µï¼Œå–ç¬¬ä¸€ä¸ª
                tokens = tokens[0]
            return self._tokenizer.convert_ids_to_tokens(tokens)
        elif isinstance(seq, str):
            # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå…ˆ tokenize å†è½¬æ¢
            input_ids = self.to_tokens(seq, prepend_bos=prepend_bos)
            return self.to_str_tokens(input_ids, prepend_bos=False)
        else:
            # å­—ç¬¦ä¸²åˆ—è¡¨
            result = []
            for s in seq:
                tokens = self.to_str_tokens(s, prepend_bos=prepend_bos)
                result.extend(tokens)
            return result
    
    def run_with_cache(
        self,
        tokens: torch.Tensor,
        remove_batch_dim: bool = True
    ) -> tuple:
        """
        è¿è¡Œæ¨¡å‹å¹¶è·å– cacheï¼Œå…¼å®¹ TransformerLens çš„ run_with_cache æ–¹æ³•
        
        Args:
            tokens: Token IDs tensor
            remove_batch_dim: æ˜¯å¦ç§»é™¤ batch ç»´åº¦ï¼ˆå½“ batch_size=1 æ—¶ï¼‰
        
        Returns:
            Tuple of (logits, cache)
            æ³¨æ„ï¼šè¿”å›çš„ cache æ˜¯ ActivationCache å¯¹è±¡ï¼Œæ”¯æŒ cache["pattern", layer, "attn"] è®¿é—®
        """
        logits, cache, attentions = run_with_cache(
            self._model,
            tokens,
            remove_batch_dim=remove_batch_dim,
            output_attentions=True,
            attn_mode="torch"  # ä½¿ç”¨ torch æ¨¡å¼è·å– attention weights
        )
        
        # è¿”å› (logits, cache)ï¼Œä¸ TransformerLens ä¿æŒä¸€è‡´
        return logits, cache
    
    @classmethod
    def from_pretrained(
        cls,
        model_name: str,
        device: str = "cuda",
        torch_dtype: torch.dtype = None,
        model_type: Optional[str] = None,  # "llama" or "gptneox" or None (auto-detect)
        **kwargs
    ):
        """
        ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½å¹¶åˆ›å»ºé€‚é…å™¨ï¼Œå…¼å®¹ TransformerLens çš„ from_pretrained æ–¹æ³•
        
        Args:
            model_name: æ¨¡å‹åç§°æˆ–è·¯å¾„
            device: è®¾å¤‡ ("cuda" æˆ– "cpu")
            torch_dtype: æ•°æ®ç±»å‹ï¼ˆå¦‚ torch.bfloat16, torch.float16ï¼‰
            model_type: æ¨¡å‹ç±»å‹ ("llama" æˆ– "gptneox")ï¼Œå¦‚æœä¸º None åˆ™è‡ªåŠ¨æ£€æµ‹
            **kwargs: ä¼ é€’ç»™æ¨¡å‹ from_pretrained çš„å…¶ä»–å‚æ•°
        
        Returns:
            CustomModelAdapter å®ä¾‹
        """
        # è‡ªåŠ¨æ£€æµ‹æ¨¡å‹ç±»å‹
        if model_type is None:
            model_name_lower = model_name.lower()
            if "pythia" in model_name_lower or "gpt-neox" in model_name_lower or "gptneox" in model_name_lower:
                model_type = "gptneox"
            elif "llama" in model_name_lower:
                model_type = "llama"
            else:
                # å°è¯•æ ¹æ®æ¨¡å‹é…ç½®åˆ¤æ–­
                try:
                    from transformers import AutoConfig
                    config = AutoConfig.from_pretrained(model_name, **{k: v for k, v in kwargs.items() if k in ['revision', 'trust_remote_code']})
                    if config.model_type == "gpt_neox":
                        model_type = "gptneox"
                    elif config.model_type == "llama":
                        model_type = "llama"
                    else:
                        # é»˜è®¤å°è¯• LLaMAï¼ˆå¦‚æœè‡ªå®šä¹‰æ¨¡å‹å¯ç”¨ï¼‰
                        if HAS_CUSTOM_LLAMA:
                            model_type = "llama"
                        else:
                            model_type = "gptneox"
                except:
                    # å¦‚æœæ— æ³•æ£€æµ‹ï¼Œé»˜è®¤ä½¿ç”¨ GPTNeoXï¼ˆæ ‡å‡† HuggingFaceï¼‰
                    model_type = "gptneox"
        
        # é»˜è®¤å‚æ•°
        default_kwargs = {
            'torch_dtype': torch_dtype or (torch.bfloat16 if device == "cuda" else torch.float32),
            'device_map': 'auto' if device == "cuda" else None,
        }
        
        # æ ¹æ®æ¨¡å‹ç±»å‹è®¾ç½®ç‰¹å®šå‚æ•°
        if model_type == "llama":
            if not HAS_CUSTOM_LLAMA:
                raise ImportError(
                    "LLaMA æ¨¡å‹éœ€è¦è‡ªå®šä¹‰å®ç°ï¼Œä½†æ— æ³•å¯¼å…¥ CustomLlamaForCausalLMã€‚"
                    "è¯·ç¡®ä¿ source.modeling_llama æ¨¡å—å¯ç”¨ã€‚"
                )
            # LLaMA ç‰¹å®šå‚æ•°
            default_kwargs['use_flash_attention_2'] = kwargs.get('use_flash_attention_2', "flash_attention_2")
            if 'use_flash_attention_2' in kwargs:
                default_kwargs['use_flash_attention_2'] = kwargs['use_flash_attention_2']
        
        default_kwargs.update(kwargs)
        
        # åŠ è½½æ¨¡å‹
        if model_type == "llama":
            model = CustomLlamaForCausalLM.from_pretrained(
                model_name,
                **default_kwargs
            )
            
            # âš ï¸ LLaMA ç‰¹å®šä¿®å¤: Flash Attention 2 å¤„ç†
            if hasattr(model.config, '_attn_implementation'):
                if model.config._attn_implementation == "eager":
                    import warnings
                    warnings.warn(
                        "Model was loaded with eager attention implementation, but custom model code requires "
                        "LlamaFlashAttention2 class (which has forward_torch method). "
                        "Forcing use of flash_attention_2 implementation. "
                        "You can still control whether to use Flash Attention via attn_mode parameter."
                    )
                    model.config._attn_implementation = "flash_attention_2"
                    
                    try:
                        from source.modeling_llama import LLAMA_ATTENTION_CLASSES
                    except ImportError:
                        import warnings
                        warnings.warn(
                            "æ— æ³•å¯¼å…¥ LLAMA_ATTENTION_CLASSESï¼Œè·³è¿‡ attention æ¨¡å—ä¿®å¤ã€‚"
                            "æ¨¡å‹å¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œã€‚"
                        )
                        model.eval()
                        return cls(model)
                    
                    model_device = next(model.parameters()).device
                    model_dtype = next(model.parameters()).dtype
                    
                    for layer_idx, layer in enumerate(model.model.layers):
                        if hasattr(layer, 'self_attn'):
                            old_attn = layer.self_attn
                            old_device = next(old_attn.parameters()).device
                            old_dtype = next(old_attn.parameters()).dtype
                            
                            new_attn = LLAMA_ATTENTION_CLASSES["flash_attention_2"](
                                config=model.config,
                                layer_idx=layer_idx
                            )
                            new_attn.load_state_dict(old_attn.state_dict())
                            new_attn = new_attn.to(device=old_device, dtype=old_dtype)
                            layer.self_attn = new_attn
        else:  # GPTNeoX
            # GPTNeoX ä½¿ç”¨æ ‡å‡† HuggingFace å®ç°ï¼Œä¸éœ€è¦ç‰¹æ®Šå¤„ç†
            model = GPTNeoXForCausalLM.from_pretrained(
                model_name,
                **default_kwargs
            )
        
        # å¦‚æœä¸æ˜¯è‡ªåŠ¨è®¾å¤‡æ˜ å°„ï¼Œæ‰‹åŠ¨ç§»åŠ¨åˆ°è®¾å¤‡
        if default_kwargs.get('device_map') is None:
            model = model.to(device)
        
        model.eval()
        
        # åˆ›å»ºé€‚é…å™¨
        return cls(model)
    
    def __call__(self, *args, **kwargs):
        """ç›´æ¥è°ƒç”¨åº•å±‚æ¨¡å‹"""
        return self._model(*args, **kwargs)
    
    def __getattr__(self, name):
        """è½¬å‘å…¶ä»–å±æ€§è®¿é—®åˆ°åº•å±‚æ¨¡å‹"""
        # tokenizer å·²ç»åœ¨ __init__ ä¸­è®¾ç½®ä¸ºå…¬å…±å±æ€§ self.tokenizer
        # å¦‚æœè®¿é—® tokenizerï¼Œåº”è¯¥ç›´æ¥è¿”å›ï¼ˆä¸åº”è¯¥åˆ°è¾¾è¿™é‡Œï¼‰
        # ä½†å¦‚æœç¡®å®åˆ°è¾¾è¿™é‡Œï¼Œè¯´æ˜ tokenizer å±æ€§å¯èƒ½ä¸å­˜åœ¨ï¼Œå°è¯•è¿”å›å®ƒ
        if name == "tokenizer":
            # å¦‚æœ self.tokenizer ä¸å­˜åœ¨ï¼Œå°è¯•è¿”å› _tokenizer
            if hasattr(self, '_tokenizer'):
                return self._tokenizer
            raise AttributeError(f"'{self.__class__.__name__}' object has no attribute 'tokenizer'")
        # å…¶ä»–å±æ€§è½¬å‘åˆ°åº•å±‚æ¨¡å‹
        return getattr(self._model, name)


# ä¸ºäº†å…¼å®¹æ€§ï¼Œä¹Ÿå¯ä»¥åˆ›å»ºä¸€ä¸ªåˆ«å
HookedTransformer = CustomModelAdapter

